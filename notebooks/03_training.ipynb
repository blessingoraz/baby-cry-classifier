{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blessingoraz/baby-cry-classifier/blob/main/03_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoHWfbHAxpY8"
      },
      "source": [
        "CNN for audio files\n",
        "- Do some processing\n",
        "- Check audio lengths/duration and sample rate\n",
        "- How do you detect noise?\n",
        "- Convert audio to images(checkout mel)\n",
        "- Split dataset to training and test\n",
        "\n",
        "Training\n",
        "- Transfer learning\n",
        "- Adjusting learning rate\n",
        "- check-pointing\n",
        "- Regularization and Dropout\n",
        "- Data Augmentation\n",
        "- Training a larger model\n",
        "Using the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCsK6GRD_eoi",
        "outputId": "4cc4a6cb-87cf-4ddf-9686-6ff9626efa16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mb5cJpHA1_F",
        "outputId": "68af4ef3-6ba8-456f-cdf5-50dd948fc716"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((306, 1, 128, 219), (306,))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "X_train = np.load(\"processed/X_train.npy\")  # (N, 1, n_mels, time)\n",
        "y_train = np.load(\"processed/y_train.npy\")  # (N,)\n",
        "X_val   = np.load(\"processed/X_val.npy\")\n",
        "y_val   = np.load(\"processed/y_val.npy\")\n",
        "X_test  = np.load(\"processed/X_test.npy\")\n",
        "y_test  = np.load(\"processed/y_test.npy\")\n",
        "\n",
        "X_train.shape, y_train.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WicY31qFP3ac"
      },
      "source": [
        "## Dataset + DataLoader (with SpecAugment)\n",
        "\n",
        "We’ll do simple augmentation only in training:\n",
        "\n",
        "Random time masking\n",
        "\n",
        "Random freq masking\n",
        "\n",
        "These work great for spectrograms and are easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4vsmOAb1oW3q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BabyCryNpyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    - Instead of loading PIL images from disk, we load spectrogram tensors from numpy arrays.\n",
        "    - 'transform' works like torchvision transforms: it modifies the spectrogram before returning it.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = X  # numpy array: (N, 1, n_mels, time)\n",
        "        self.y = y  # numpy array: (N,)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec = torch.tensor(self.X[idx], dtype=torch.float32)  # (1, n_mels, time)\n",
        "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            spec = self.transform(spec)\n",
        "\n",
        "        return spec, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Zlfre_31FTpG"
      },
      "outputs": [],
      "source": [
        "# Compose (like torchvision.transforms.Compose)\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1-VpnEQ3FiNu"
      },
      "outputs": [],
      "source": [
        "# Simple SpecAugment transforms\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "class SpecAugment:\n",
        "    def __init__(self, freq_mask=12, time_mask=24, p=0.7):\n",
        "        self.freq = T.FrequencyMasking(freq_mask_param=freq_mask)\n",
        "        self.time = T.TimeMasking(time_mask_param=time_mask)\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: (1, n_mels, time)\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            x = self.freq(x)\n",
        "            x = self.time(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CwKS6tvaFseO"
      },
      "outputs": [],
      "source": [
        "# Resize to 224x224\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResizeSpec:\n",
        "    def __init__(self, height=224, width=224):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: (1, H, W)\n",
        "        x = x.unsqueeze(0)  # (1,1,H,W)\n",
        "        x = F.interpolate(x, size=(self.height, self.width), mode=\"bilinear\", align_corners=False)\n",
        "        return x.squeeze(0)  # (1, height, width)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hn-paxKGBY5"
      },
      "source": [
        "### Create train/val transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Um6EuyNyF8aN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Train: resize + augmentation\n",
        "\n",
        "Validation: resize only\n",
        "\n",
        "Test: resize only\n",
        "'''\n",
        "train_transforms = Compose([\n",
        "    ResizeSpec(224, 224),     # like transforms.Resize\n",
        "    SpecAugment(freq_mask=12, time_mask=24, p=0.7)  # augmentation\n",
        "])\n",
        "\n",
        "val_transforms = Compose([\n",
        "    ResizeSpec(224, 224)\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    ResizeSpec(224, 224)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnI3CKH2GkNd"
      },
      "source": [
        "### Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1xhV6MBGjQ1",
        "outputId": "ecc1aa4a-ff46-47c0-cee2-e628c2a0968b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 1, 224, 224]), torch.Size([32]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "class_sample_count = np.bincount(y_train)\n",
        "weights_per_class = 1.0 / class_sample_count\n",
        "sample_weights = weights_per_class[y_train]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.tensor(sample_weights, dtype=torch.double),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "train_dataset = BabyCryNpyDataset(X_train, y_train, transform=train_transforms)\n",
        "val_dataset   = BabyCryNpyDataset(X_val, y_val, transform=val_transforms)\n",
        "test_dataset  = BabyCryNpyDataset(X_test, y_test, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# sanity check\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb.shape, yb.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jhal0Rh_oixX"
      },
      "source": [
        "### Handle class imbalance (weighted loss)\n",
        "\n",
        "Because the dataset is highly imbalanced, I used class-weighted cross-entropy to penalize misclassification of minority classes more heavily. This encourages the model to learn discriminative features for all cry types rather than overfitting to the majority class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ib4ZGsloqHr",
        "outputId": "a2f958be-113f-4f2c-cd6b-91fed8d0773b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.8250, 3.8250, 7.6500, 2.1250, 0.1678, 5.4643, 3.1875, 2.3906])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "counts = Counter(y_train.tolist())\n",
        "num_classes = len(set(y_train.tolist()))\n",
        "\n",
        "class_counts = np.array([counts[i] for i in range(num_classes)], dtype=np.float32)\n",
        "class_weights = class_counts.sum() / (num_classes * class_counts)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "class_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK21ZShDTLXT",
        "outputId": "d5edae9e-f2cc-482f-ae12-1b2d08f63e79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val: Counter({4: 77, 3: 6, 7: 6, 1: 4, 6: 4, 0: 3, 5: 2, 2: 1})\n",
            "test: Counter({4: 77, 7: 6, 3: 6, 1: 4, 6: 4, 0: 3, 5: 2, 2: 1})\n"
          ]
        }
      ],
      "source": [
        "print(\"val:\", Counter(y_val.tolist()))\n",
        "print(\"test:\", Counter(y_test.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZxlZiLZKX0Z",
        "outputId": "0bb13595-f217-4a45-d06d-a6ce04aa6d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class 0: weight=3.83, count=10\n",
            "Class 1: weight=3.83, count=10\n",
            "Class 2: weight=7.65, count=5\n",
            "Class 3: weight=2.12, count=18\n",
            "Class 4: weight=0.17, count=228\n",
            "Class 5: weight=5.46, count=7\n",
            "Class 6: weight=3.19, count=12\n",
            "Class 7: weight=2.39, count=16\n"
          ]
        }
      ],
      "source": [
        "for i, w in enumerate(class_weights):\n",
        "    print(f\"Class {i}: weight={w:.2f}, count={counts[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vv4K27MQXDY"
      },
      "source": [
        "### Transfer Learning Model (ResNet)\n",
        "\n",
        "I chose ResNet18 as my baseline because it provides a strong balance between model capacity and stability, which is especially important for small, imbalanced datasets. Its residual connections help prevent overfitting and make transfer learning more effective on spectrogram-based audio data.\n",
        "\n",
        "We also add Dropout before the classifier head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2-cyf_TBgBS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CryResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        backbone=\"resnet18\",\n",
        "        pretrained=True,\n",
        "        freeze_backbone=True,\n",
        "        droprate=0.2,\n",
        "        size_inner=512):    # <- changed default from 256 to 512\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained Resnet18\n",
        "        if backbone == \"resnet18\":\n",
        "            self.base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "        elif backbone == \"resnet34\":\n",
        "            self.base_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "        else:\n",
        "            raise ValueError(\"backbone must be resnet18 or resnet34\")\n",
        "\n",
        "        # Freeze base model parameters\n",
        "        if freeze_backbone:\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Replace classifier head\n",
        "        in_features = self.base_model.fc.in_features\n",
        "\n",
        "        self.base_model.fc = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(droprate),\n",
        "            nn.Linear(in_features, size_inner),\n",
        "            nn.Linear(size_inner, num_classes)\n",
        "        )\n",
        "\n",
        "        # Ensure head is trainable (safe even if freeze_backbone=True)\n",
        "        for p in self.base_model.fc.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, n_mels, time) -> convert to 3-channel\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        return self.base_model(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiF1nWwlAzJn"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwYeMbSdAynQ",
        "outputId": "e355424d-8d26-48b1-804c-1bbd951a3fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CryResNet(num_classes=num_classes, backbone=\"resnet18\", pretrained=True, droprate=0.2)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.5, patience=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG45AyIMLnHp"
      },
      "source": [
        "I used a ReduceLROnPlateau scheduler to automatically lower the learning rate when validation loss stopped improving, allowing for more stable fine-tuning on a small and imbalanced dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZxVaWlTRTU0"
      },
      "source": [
        "### Training + validation loops\n",
        "Because the dataset is highly imbalanced, accuracy alone is insufficient. I therefore tracked macro-F1 and per-class recall during validation to ensure that minority cry categories were not ignored.\n",
        "\n",
        "NB: Do this later:\n",
        "- use macro F1\n",
        "- summarize results in a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gEI-RoNLBktR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def train_and_evaluate(\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    ckpt_dir=\"models\",\n",
        "    ckpt_name=\"best.pt\"):\n",
        "\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    best_val_macro_f1 = 0.0\n",
        "    best_epoch = 0\n",
        "    best_path = os.path.join(ckpt_dir, ckpt_name)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Collect for macro-F1\n",
        "                all_preds.append(predicted.detach().cpu().numpy())\n",
        "                all_labels.append(labels.detach().cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        y_pred = np.concatenate(all_preds)\n",
        "        y_true = np.concatenate(all_labels)\n",
        "\n",
        "        val_macro_f1 = f1_score(\n",
        "            y_true,\n",
        "            y_pred,\n",
        "            average=\"macro\",\n",
        "            labels=list(range(num_classes))  # ensures stable even if a class missing in val batch\n",
        "        )\n",
        "\n",
        "        # Scheduler watches val_loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # ===== CHECKPOINT =====\n",
        "        if val_macro_f1 > best_val_macro_f1:\n",
        "            best_val_macro_f1 = val_macro_f1\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "            torch.save({\n",
        "                \"epoch\": best_epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"best_val_macro_f1\": best_val_macro_f1,\n",
        "            }, best_path)\n",
        "\n",
        "            print(f\"  ✅ Saved new best macro-F1={best_val_macro_f1:.4f} to {best_path} (epoch {best_epoch})\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return best_val_macro_f1, best_epoch, best_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pYHtg74Tizk"
      },
      "source": [
        "### Tuning the Learning Rate\n",
        "- Try multiple values: [0.0001, 0.001, 0.01, 0.1]\n",
        "- Train for a few epochs each\n",
        "- Compare validation accuracy\n",
        "- Choose the rate with best performance and smallest train/val gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bz73t9e3TmWG"
      },
      "outputs": [],
      "source": [
        "def make_model(learning_rate=0.001):\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    model = CryResNet(num_classes=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApcDjU3CURnJ"
      },
      "source": [
        "Testing different learning rates:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2PZWt_NvF-uF"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wLLCyNJJUVJf",
        "outputId": "025c5303-47c8-4f40-e3c7-fb0bf21adcef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Learning Rate: 0.0001 ===\n",
            "Number of classes: 8\n",
            "  ✅ Saved new best macro-F1=0.0433 to models/best_lr_0.0001.pt (epoch 1)\n",
            "Epoch 1/20\n",
            "  Train Loss: 2.1426, Train Acc: 0.0588\n",
            "  Val   Loss: 2.0948, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.0592 to models/best_lr_0.0001.pt (epoch 2)\n",
            "Epoch 2/20\n",
            "  Train Loss: 2.0312, Train Acc: 0.2288\n",
            "  Val   Loss: 2.0641, Val   Acc: 0.1553\n",
            "  ✅ Saved new best macro-F1=0.1354 to models/best_lr_0.0001.pt (epoch 3)\n",
            "Epoch 3/20\n",
            "  Train Loss: 2.1023, Train Acc: 0.4510\n",
            "  Val   Loss: 2.0950, Val   Acc: 0.6408\n",
            "Epoch 4/20\n",
            "  Train Loss: 2.0490, Train Acc: 0.3856\n",
            "  Val   Loss: 2.1019, Val   Acc: 0.5049\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.9591, Train Acc: 0.1928\n",
            "  Val   Loss: 2.0718, Val   Acc: 0.2524\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.9546, Train Acc: 0.1830\n",
            "  Val   Loss: 2.0457, Val   Acc: 0.3592\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.9072, Train Acc: 0.2549\n",
            "  Val   Loss: 2.0255, Val   Acc: 0.3786\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.9139, Train Acc: 0.2353\n",
            "  Val   Loss: 2.0056, Val   Acc: 0.2913\n",
            "  ✅ Saved new best macro-F1=0.1677 to models/best_lr_0.0001.pt (epoch 9)\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.9204, Train Acc: 0.2778\n",
            "  Val   Loss: 2.0161, Val   Acc: 0.3981\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.8468, Train Acc: 0.3105\n",
            "  Val   Loss: 2.0154, Val   Acc: 0.3301\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.8651, Train Acc: 0.2941\n",
            "  Val   Loss: 2.0294, Val   Acc: 0.3107\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.8150, Train Acc: 0.3399\n",
            "  Val   Loss: 2.0362, Val   Acc: 0.2524\n",
            "Epoch 13/20\n",
            "  Train Loss: 1.8505, Train Acc: 0.3366\n",
            "  Val   Loss: 2.0395, Val   Acc: 0.2621\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.8433, Train Acc: 0.3464\n",
            "  Val   Loss: 2.0444, Val   Acc: 0.3204\n",
            "Epoch 15/20\n",
            "  Train Loss: 1.8559, Train Acc: 0.3268\n",
            "  Val   Loss: 2.0465, Val   Acc: 0.3010\n",
            "Epoch 16/20\n",
            "  Train Loss: 1.7607, Train Acc: 0.3301\n",
            "  Val   Loss: 2.0458, Val   Acc: 0.2913\n",
            "Epoch 17/20\n",
            "  Train Loss: 1.7917, Train Acc: 0.3268\n",
            "  Val   Loss: 2.0416, Val   Acc: 0.2816\n",
            "Epoch 18/20\n",
            "  Train Loss: 1.8183, Train Acc: 0.3105\n",
            "  Val   Loss: 2.0391, Val   Acc: 0.2621\n",
            "Epoch 19/20\n",
            "  Train Loss: 1.7585, Train Acc: 0.3464\n",
            "  Val   Loss: 2.0408, Val   Acc: 0.2718\n",
            "Epoch 20/20\n",
            "  Train Loss: 1.8669, Train Acc: 0.2810\n",
            "  Val   Loss: 2.0396, Val   Acc: 0.2913\n",
            "\n",
            "=== Learning Rate: 0.001 ===\n",
            "Number of classes: 8\n",
            "  ✅ Saved new best macro-F1=0.0224 to models/best_lr_0.001.pt (epoch 1)\n",
            "Epoch 1/20\n",
            "  Train Loss: 2.3913, Train Acc: 0.1438\n",
            "  Val   Loss: 2.1075, Val   Acc: 0.0485\n",
            "  ✅ Saved new best macro-F1=0.0985 to models/best_lr_0.001.pt (epoch 2)\n",
            "Epoch 2/20\n",
            "  Train Loss: 2.0011, Train Acc: 0.2745\n",
            "  Val   Loss: 2.4151, Val   Acc: 0.6311\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.7700, Train Acc: 0.3824\n",
            "  Val   Loss: 2.2211, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.1649 to models/best_lr_0.001.pt (epoch 4)\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.5248, Train Acc: 0.2190\n",
            "  Val   Loss: 2.2305, Val   Acc: 0.5728\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.2831, Train Acc: 0.4118\n",
            "  Val   Loss: 2.1544, Val   Acc: 0.3398\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.2859, Train Acc: 0.4314\n",
            "  Val   Loss: 2.2652, Val   Acc: 0.4078\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.2031, Train Acc: 0.3725\n",
            "  Val   Loss: 2.1322, Val   Acc: 0.1262\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.0739, Train Acc: 0.2908\n",
            "  Val   Loss: 2.1554, Val   Acc: 0.1845\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.2861, Train Acc: 0.5163\n",
            "  Val   Loss: 2.2540, Val   Acc: 0.4272\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.0375, Train Acc: 0.4837\n",
            "  Val   Loss: 2.1697, Val   Acc: 0.1359\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.0811, Train Acc: 0.3562\n",
            "  Val   Loss: 2.2155, Val   Acc: 0.1845\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.0909, Train Acc: 0.3987\n",
            "  Val   Loss: 2.2427, Val   Acc: 0.2330\n",
            "Epoch 13/20\n",
            "  Train Loss: 1.0833, Train Acc: 0.4837\n",
            "  Val   Loss: 2.2788, Val   Acc: 0.3495\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.0338, Train Acc: 0.5719\n",
            "  Val   Loss: 2.2895, Val   Acc: 0.4078\n",
            "Epoch 15/20\n",
            "  Train Loss: 1.0602, Train Acc: 0.5523\n",
            "  Val   Loss: 2.2830, Val   Acc: 0.3689\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.9285, Train Acc: 0.5033\n",
            "  Val   Loss: 2.2896, Val   Acc: 0.2427\n",
            "Epoch 17/20\n",
            "  Train Loss: 1.0203, Train Acc: 0.4706\n",
            "  Val   Loss: 2.2818, Val   Acc: 0.2233\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.9885, Train Acc: 0.4510\n",
            "  Val   Loss: 2.2784, Val   Acc: 0.2330\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.9757, Train Acc: 0.4477\n",
            "  Val   Loss: 2.2786, Val   Acc: 0.2816\n",
            "Epoch 20/20\n",
            "  Train Loss: 1.1694, Train Acc: 0.4379\n",
            "  Val   Loss: 2.2807, Val   Acc: 0.3107\n",
            "\n",
            "=== Learning Rate: 0.01 ===\n",
            "Number of classes: 8\n",
            "  ✅ Saved new best macro-F1=0.0028 to models/best_lr_0.01.pt (epoch 1)\n",
            "Epoch 1/20\n",
            "  Train Loss: 15.6135, Train Acc: 0.1307\n",
            "  Val   Loss: 10.1723, Val   Acc: 0.0097\n",
            "  ✅ Saved new best macro-F1=0.1069 to models/best_lr_0.01.pt (epoch 2)\n",
            "Epoch 2/20\n",
            "  Train Loss: 11.3494, Train Acc: 0.0784\n",
            "  Val   Loss: 11.3701, Val   Acc: 0.7476\n",
            "Epoch 3/20\n",
            "  Train Loss: 6.6473, Train Acc: 0.3039\n",
            "  Val   Loss: 5.9902, Val   Acc: 0.0194\n",
            "Epoch 4/20\n",
            "  Train Loss: 3.1279, Train Acc: 0.2647\n",
            "  Val   Loss: 4.1462, Val   Acc: 0.0583\n",
            "Epoch 5/20\n",
            "  Train Loss: 2.0963, Train Acc: 0.1209\n",
            "  Val   Loss: 3.3216, Val   Acc: 0.1068\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.7712, Train Acc: 0.3595\n",
            "  Val   Loss: 2.8672, Val   Acc: 0.0680\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.4900, Train Acc: 0.2974\n",
            "  Val   Loss: 3.3055, Val   Acc: 0.2039\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.1885, Train Acc: 0.2745\n",
            "  Val   Loss: 3.6713, Val   Acc: 0.2718\n",
            "  ✅ Saved new best macro-F1=0.1212 to models/best_lr_0.01.pt (epoch 9)\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.4529, Train Acc: 0.3595\n",
            "  Val   Loss: 3.3758, Val   Acc: 0.2913\n",
            "  ✅ Saved new best macro-F1=0.1313 to models/best_lr_0.01.pt (epoch 10)\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.1773, Train Acc: 0.3301\n",
            "  Val   Loss: 3.1832, Val   Acc: 0.1553\n",
            "  ✅ Saved new best macro-F1=0.1503 to models/best_lr_0.01.pt (epoch 11)\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.1325, Train Acc: 0.4216\n",
            "  Val   Loss: 2.9695, Val   Acc: 0.3301\n",
            "  ✅ Saved new best macro-F1=0.1534 to models/best_lr_0.01.pt (epoch 12)\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.0344, Train Acc: 0.3791\n",
            "  Val   Loss: 2.8442, Val   Acc: 0.2233\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.9691, Train Acc: 0.5458\n",
            "  Val   Loss: 3.1524, Val   Acc: 0.2621\n",
            "  ✅ Saved new best macro-F1=0.1862 to models/best_lr_0.01.pt (epoch 14)\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.1040, Train Acc: 0.3464\n",
            "  Val   Loss: 3.1410, Val   Acc: 0.4369\n",
            "Epoch 15/20\n",
            "  Train Loss: 1.0281, Train Acc: 0.5261\n",
            "  Val   Loss: 3.0169, Val   Acc: 0.1942\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.8158, Train Acc: 0.3791\n",
            "  Val   Loss: 3.0421, Val   Acc: 0.4272\n",
            "  ✅ Saved new best macro-F1=0.1943 to models/best_lr_0.01.pt (epoch 17)\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.7834, Train Acc: 0.5686\n",
            "  Val   Loss: 2.9995, Val   Acc: 0.3107\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.6816, Train Acc: 0.5033\n",
            "  Val   Loss: 3.0505, Val   Acc: 0.4660\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.6807, Train Acc: 0.6275\n",
            "  Val   Loss: 3.0252, Val   Acc: 0.4660\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.9510, Train Acc: 0.5196\n",
            "  Val   Loss: 2.9479, Val   Acc: 0.3301\n",
            "\n",
            "=== Learning Rate: 0.1 ===\n",
            "Number of classes: 8\n",
            "  ✅ Saved new best macro-F1=0.0114 to models/best_lr_0.1.pt (epoch 1)\n",
            "Epoch 1/20\n",
            "  Train Loss: 941.0653, Train Acc: 0.1144\n",
            "  Val   Loss: 578.6197, Val   Acc: 0.0388\n",
            "  ✅ Saved new best macro-F1=0.0568 to models/best_lr_0.1.pt (epoch 2)\n",
            "Epoch 2/20\n",
            "  Train Loss: 590.9280, Train Acc: 0.1961\n",
            "  Val   Loss: 450.5454, Val   Acc: 0.2136\n",
            "  ✅ Saved new best macro-F1=0.1075 to models/best_lr_0.1.pt (epoch 3)\n",
            "Epoch 3/20\n",
            "  Train Loss: 386.0115, Train Acc: 0.1111\n",
            "  Val   Loss: 269.5970, Val   Acc: 0.2913\n",
            "Epoch 4/20\n",
            "  Train Loss: 197.3957, Train Acc: 0.1928\n",
            "  Val   Loss: 396.8467, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.1116 to models/best_lr_0.1.pt (epoch 5)\n",
            "Epoch 5/20\n",
            "  Train Loss: 167.0013, Train Acc: 0.2908\n",
            "  Val   Loss: 282.0328, Val   Acc: 0.4660\n",
            "  ✅ Saved new best macro-F1=0.1683 to models/best_lr_0.1.pt (epoch 6)\n",
            "Epoch 6/20\n",
            "  Train Loss: 136.4243, Train Acc: 0.1863\n",
            "  Val   Loss: 193.7550, Val   Acc: 0.6408\n",
            "Epoch 7/20\n",
            "  Train Loss: 106.0523, Train Acc: 0.2386\n",
            "  Val   Loss: 220.4738, Val   Acc: 0.0485\n",
            "Epoch 8/20\n",
            "  Train Loss: 65.8780, Train Acc: 0.2908\n",
            "  Val   Loss: 205.2601, Val   Acc: 0.7184\n",
            "Epoch 9/20\n",
            "  Train Loss: 84.6383, Train Acc: 0.3301\n",
            "  Val   Loss: 192.7478, Val   Acc: 0.3010\n",
            "Epoch 10/20\n",
            "  Train Loss: 45.2473, Train Acc: 0.3954\n",
            "  Val   Loss: 161.9558, Val   Acc: 0.1942\n",
            "Epoch 11/20\n",
            "  Train Loss: 50.1043, Train Acc: 0.3693\n",
            "  Val   Loss: 169.1097, Val   Acc: 0.1942\n",
            "Epoch 12/20\n",
            "  Train Loss: 62.3090, Train Acc: 0.4542\n",
            "  Val   Loss: 173.2763, Val   Acc: 0.0680\n",
            "Epoch 13/20\n",
            "  Train Loss: 46.6550, Train Acc: 0.3399\n",
            "  Val   Loss: 154.4521, Val   Acc: 0.3689\n",
            "Epoch 14/20\n",
            "  Train Loss: 32.5970, Train Acc: 0.3758\n",
            "  Val   Loss: 149.5938, Val   Acc: 0.2233\n",
            "Epoch 15/20\n",
            "  Train Loss: 57.7931, Train Acc: 0.3824\n",
            "  Val   Loss: 168.6179, Val   Acc: 0.2621\n",
            "  ✅ Saved new best macro-F1=0.2083 to models/best_lr_0.1.pt (epoch 16)\n",
            "Epoch 16/20\n",
            "  Train Loss: 79.3041, Train Acc: 0.3301\n",
            "  Val   Loss: 168.5934, Val   Acc: 0.5340\n",
            "Epoch 17/20\n",
            "  Train Loss: 71.2396, Train Acc: 0.3235\n",
            "  Val   Loss: 197.4542, Val   Acc: 0.4369\n",
            "Epoch 18/20\n",
            "  Train Loss: 49.9662, Train Acc: 0.2418\n",
            "  Val   Loss: 167.9874, Val   Acc: 0.4854\n",
            "Epoch 19/20\n",
            "  Train Loss: 34.0548, Train Acc: 0.4281\n",
            "  Val   Loss: 133.4895, Val   Acc: 0.1942\n",
            "  ✅ Saved new best macro-F1=0.2280 to models/best_lr_0.1.pt (epoch 20)\n",
            "Epoch 20/20\n",
            "  Train Loss: 52.0655, Train Acc: 0.4444\n",
            "  Val   Loss: 167.6485, Val   Acc: 0.5146\n",
            "Best val macro acc=0.2280 at epoch 20 | saved: models/best_lr_0.1.pt\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f'\\n=== Learning Rate: {lr} ===')\n",
        "    set_seed(42)\n",
        "    model, optimizer = make_model(learning_rate=lr)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "\n",
        "    ckpt_name = f\"best_lr_{lr}.pt\"\n",
        "\n",
        "    best_val_macro_f1, best_epoch, best_path = train_and_evaluate(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        num_epochs=20,\n",
        "        device=device,\n",
        "        ckpt_dir=\"models\",\n",
        "        ckpt_name=ckpt_name\n",
        "    )\n",
        "\n",
        "print(f\"Best val macro-F1={best_val_macro_f1:.4f} at epoch {best_epoch} | saved: {best_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4jKmhWmw1A"
      },
      "source": [
        "Upsate this: The best learning rate is 0.1 (fi score 0.2280)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGMDvb9fPSYZ",
        "outputId": "c9ca2ee1-a877-41df-9474-6fe6f127a2c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ckpt: 20 0.22802832811956897\n"
          ]
        }
      ],
      "source": [
        "ckpt = torch.load(best_path, map_location=device)\n",
        "print(\"Loaded ckpt:\", ckpt[\"epoch\"], ckpt[\"best_val_macro_f1\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhsLangtRxOp"
      },
      "source": [
        " ### Adding Inner Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4kokXCnr75H"
      },
      "source": [
        "In ResNet, the model already does pooling + flatten internally, so you can just replace the fc head with a small MLP (inner layer + ReLU + dropout + output)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sslYpmTisRzu"
      },
      "outputs": [],
      "source": [
        "def make_model(learning_rate=0.01, size_inner=256):\n",
        "    model = CryResNet(\n",
        "        num_classes=num_classes,\n",
        "        size_inner=size_inner,\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=learning_rate,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "    return model, optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SxyyASXXs8WJ",
        "outputId": "55a04d32-b0ca-4a1f-bb91-90a080e85870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== inner=64 ===\n",
            "  ✅ Saved new best macro-F1=0.0071 to models/best_lr_0.01_inner_64.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 7.1753, Train Acc: 0.2124\n",
            "  Val   Loss: 4.6139, Val   Acc: 0.0291\n",
            "  ✅ Saved new best macro-F1=0.0211 to models/best_lr_0.01_inner_64.pt (epoch 2)\n",
            "Epoch 2/10\n",
            "  Train Loss: 3.5493, Train Acc: 0.2124\n",
            "  Val   Loss: 3.6255, Val   Acc: 0.0485\n",
            "  ✅ Saved new best macro-F1=0.0579 to models/best_lr_0.01_inner_64.pt (epoch 3)\n",
            "Epoch 3/10\n",
            "  Train Loss: 2.0013, Train Acc: 0.2288\n",
            "  Val   Loss: 3.0209, Val   Acc: 0.1456\n",
            "  ✅ Saved new best macro-F1=0.1230 to models/best_lr_0.01_inner_64.pt (epoch 4)\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.7522, Train Acc: 0.2680\n",
            "  Val   Loss: 3.1343, Val   Acc: 0.2136\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.7814, Train Acc: 0.2810\n",
            "  Val   Loss: 2.8569, Val   Acc: 0.2233\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.4502, Train Acc: 0.3595\n",
            "  Val   Loss: 3.1392, Val   Acc: 0.0874\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.4691, Train Acc: 0.3399\n",
            "  Val   Loss: 2.4225, Val   Acc: 0.1553\n",
            "  ✅ Saved new best macro-F1=0.1330 to models/best_lr_0.01_inner_64.pt (epoch 8)\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.2217, Train Acc: 0.4052\n",
            "  Val   Loss: 2.8391, Val   Acc: 0.3010\n",
            "  ✅ Saved new best macro-F1=0.2016 to models/best_lr_0.01_inner_64.pt (epoch 9)\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.8461, Train Acc: 0.5621\n",
            "  Val   Loss: 2.9931, Val   Acc: 0.4369\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.1334, Train Acc: 0.3922\n",
            "  Val   Loss: 2.8717, Val   Acc: 0.3786\n",
            "Best val acc=0.2016 at epoch 9 | saved: models/best_lr_0.01_inner_64.pt\n",
            "\n",
            "=== inner=128 ===\n",
            "  ✅ Saved new best macro-F1=0.0646 to models/best_lr_0.01_inner_128.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 7.9765, Train Acc: 0.1601\n",
            "  Val   Loss: 3.5582, Val   Acc: 0.2621\n",
            "Epoch 2/10\n",
            "  Train Loss: 3.4667, Train Acc: 0.2516\n",
            "  Val   Loss: 4.0435, Val   Acc: 0.0194\n",
            "  ✅ Saved new best macro-F1=0.0861 to models/best_lr_0.01_inner_128.pt (epoch 3)\n",
            "Epoch 3/10\n",
            "  Train Loss: 2.3046, Train Acc: 0.2582\n",
            "  Val   Loss: 3.9054, Val   Acc: 0.3981\n",
            "Epoch 4/10\n",
            "  Train Loss: 2.3268, Train Acc: 0.2418\n",
            "  Val   Loss: 4.7048, Val   Acc: 0.0485\n",
            "Epoch 5/10\n",
            "  Train Loss: 2.1044, Train Acc: 0.4216\n",
            "  Val   Loss: 2.8656, Val   Acc: 0.0971\n",
            "  ✅ Saved new best macro-F1=0.0871 to models/best_lr_0.01_inner_128.pt (epoch 6)\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.5370, Train Acc: 0.2190\n",
            "  Val   Loss: 3.1164, Val   Acc: 0.1845\n",
            "  ✅ Saved new best macro-F1=0.0970 to models/best_lr_0.01_inner_128.pt (epoch 7)\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.1107, Train Acc: 0.5098\n",
            "  Val   Loss: 3.5212, Val   Acc: 0.2913\n",
            "  ✅ Saved new best macro-F1=0.1033 to models/best_lr_0.01_inner_128.pt (epoch 8)\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.4648, Train Acc: 0.4608\n",
            "  Val   Loss: 2.8876, Val   Acc: 0.0874\n",
            "  ✅ Saved new best macro-F1=0.1083 to models/best_lr_0.01_inner_128.pt (epoch 9)\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.1065, Train Acc: 0.2516\n",
            "  Val   Loss: 3.0351, Val   Acc: 0.2233\n",
            "  ✅ Saved new best macro-F1=0.1568 to models/best_lr_0.01_inner_128.pt (epoch 10)\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.0925, Train Acc: 0.5556\n",
            "  Val   Loss: 3.0328, Val   Acc: 0.4951\n",
            "Best val acc=0.1568 at epoch 10 | saved: models/best_lr_0.01_inner_128.pt\n",
            "\n",
            "=== inner=256 ===\n",
            "  ✅ Saved new best macro-F1=0.0028 to models/best_lr_0.01_inner_256.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 15.6135, Train Acc: 0.1307\n",
            "  Val   Loss: 10.1723, Val   Acc: 0.0097\n",
            "  ✅ Saved new best macro-F1=0.1069 to models/best_lr_0.01_inner_256.pt (epoch 2)\n",
            "Epoch 2/10\n",
            "  Train Loss: 11.3494, Train Acc: 0.0784\n",
            "  Val   Loss: 11.3701, Val   Acc: 0.7476\n",
            "Epoch 3/10\n",
            "  Train Loss: 6.6473, Train Acc: 0.3039\n",
            "  Val   Loss: 5.9902, Val   Acc: 0.0194\n",
            "Epoch 4/10\n",
            "  Train Loss: 3.1279, Train Acc: 0.2647\n",
            "  Val   Loss: 4.1462, Val   Acc: 0.0583\n",
            "Epoch 5/10\n",
            "  Train Loss: 2.0963, Train Acc: 0.1209\n",
            "  Val   Loss: 3.3216, Val   Acc: 0.1068\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.7712, Train Acc: 0.3595\n",
            "  Val   Loss: 2.8672, Val   Acc: 0.0680\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.4900, Train Acc: 0.2974\n",
            "  Val   Loss: 3.3055, Val   Acc: 0.2039\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.1885, Train Acc: 0.2745\n",
            "  Val   Loss: 3.6713, Val   Acc: 0.2718\n",
            "  ✅ Saved new best macro-F1=0.1212 to models/best_lr_0.01_inner_256.pt (epoch 9)\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.4529, Train Acc: 0.3595\n",
            "  Val   Loss: 3.3758, Val   Acc: 0.2913\n",
            "  ✅ Saved new best macro-F1=0.1313 to models/best_lr_0.01_inner_256.pt (epoch 10)\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.1773, Train Acc: 0.3301\n",
            "  Val   Loss: 3.1832, Val   Acc: 0.1553\n",
            "Best val acc=0.1313 at epoch 10 | saved: models/best_lr_0.01_inner_256.pt\n",
            "\n",
            "=== inner=512 ===\n",
            "  ✅ Saved new best macro-F1=0.1069 to models/best_lr_0.01_inner_512.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 27.5323, Train Acc: 0.0980\n",
            "  Val   Loss: 19.4045, Val   Acc: 0.7476\n",
            "Epoch 2/10\n",
            "  Train Loss: 16.5377, Train Acc: 0.1471\n",
            "  Val   Loss: 12.1164, Val   Acc: 0.5437\n",
            "Epoch 3/10\n",
            "  Train Loss: 8.8650, Train Acc: 0.1961\n",
            "  Val   Loss: 7.9811, Val   Acc: 0.0194\n",
            "Epoch 4/10\n",
            "  Train Loss: 4.1627, Train Acc: 0.3072\n",
            "  Val   Loss: 6.5906, Val   Acc: 0.0583\n",
            "Epoch 5/10\n",
            "  Train Loss: 3.2971, Train Acc: 0.3497\n",
            "  Val   Loss: 5.7562, Val   Acc: 0.1456\n",
            "  ✅ Saved new best macro-F1=0.1281 to models/best_lr_0.01_inner_512.pt (epoch 6)\n",
            "Epoch 6/10\n",
            "  Train Loss: 2.0836, Train Acc: 0.2320\n",
            "  Val   Loss: 6.3015, Val   Acc: 0.3786\n",
            "  ✅ Saved new best macro-F1=0.1613 to models/best_lr_0.01_inner_512.pt (epoch 7)\n",
            "Epoch 7/10\n",
            "  Train Loss: 2.6539, Train Acc: 0.3007\n",
            "  Val   Loss: 6.0682, Val   Acc: 0.4272\n",
            "Epoch 8/10\n",
            "  Train Loss: 2.1256, Train Acc: 0.3497\n",
            "  Val   Loss: 4.7577, Val   Acc: 0.1359\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.7660, Train Acc: 0.3399\n",
            "  Val   Loss: 6.7324, Val   Acc: 0.0583\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.7977, Train Acc: 0.2843\n",
            "  Val   Loss: 5.0458, Val   Acc: 0.1748\n",
            "Best val acc=0.1613 at epoch 7 | saved: models/best_lr_0.01_inner_512.pt\n"
          ]
        }
      ],
      "source": [
        "inner_sizes = [64, 128, 256, 512]\n",
        "learning_rate = 0.01\n",
        "\n",
        "for size_inner in inner_sizes:\n",
        "    print(f\"\\n=== inner={size_inner} ===\")\n",
        "    set_seed(42)\n",
        "    model, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "\n",
        "    ckpt_name = f\"best_lr_{learning_rate}_inner_{size_inner}.pt\"\n",
        "\n",
        "    best_val_macro_f1, best_epoch, best_path = train_and_evaluate(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        num_epochs=10,\n",
        "        device=device,\n",
        "        ckpt_dir=\"models\",\n",
        "        ckpt_name=ckpt_name\n",
        "    )\n",
        "\n",
        "    print(f\"Best val macro-F1={best_val_macro_f1:.4f} at epoch {best_epoch} | saved: {best_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDKi7hbM057j"
      },
      "source": [
        "The best inner layer is 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3vpXbQTxBfJ"
      },
      "source": [
        "### Dropout Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "QlQ0WiDm2Jq2"
      },
      "outputs": [],
      "source": [
        "def make_model(learning_rate=0.01, size_inner=512, droprate=0.2):\n",
        "    model = CryResNet(\n",
        "        num_classes=num_classes,\n",
        "        size_inner=size_inner,\n",
        "        droprate=droprate\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=learning_rate,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8WBqdP72ifh",
        "outputId": "70d41885-a5bc-4f93-b383-891469a0f0c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Droprate=0.0 ===\n",
            "  ✅ Saved new best macro-F1=0.0048 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 24.0443, Train Acc: 0.0915\n",
            "  Val   Loss: 20.2146, Val   Acc: 0.0194\n",
            "  ✅ Saved new best macro-F1=0.0138 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 2)\n",
            "Epoch 2/10\n",
            "  Train Loss: 15.0115, Train Acc: 0.2092\n",
            "  Val   Loss: 13.0797, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.0157 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 3)\n",
            "Epoch 3/10\n",
            "  Train Loss: 7.4093, Train Acc: 0.1961\n",
            "  Val   Loss: 10.6334, Val   Acc: 0.0291\n",
            "  ✅ Saved new best macro-F1=0.0842 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 4)\n",
            "Epoch 4/10\n",
            "  Train Loss: 4.7955, Train Acc: 0.1242\n",
            "  Val   Loss: 6.0750, Val   Acc: 0.2136\n",
            "  ✅ Saved new best macro-F1=0.0865 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 5)\n",
            "Epoch 5/10\n",
            "  Train Loss: 3.8489, Train Acc: 0.4052\n",
            "  Val   Loss: 7.3523, Val   Acc: 0.0971\n",
            "  ✅ Saved new best macro-F1=0.0942 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 6)\n",
            "Epoch 6/10\n",
            "  Train Loss: 2.5784, Train Acc: 0.4020\n",
            "  Val   Loss: 6.2206, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.1316 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 7)\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.6226, Train Acc: 0.2353\n",
            "  Val   Loss: 6.2807, Val   Acc: 0.2816\n",
            "  ✅ Saved new best macro-F1=0.1452 to models/best_lr_0.01_inner_512_drop_0.0.pt (epoch 8)\n",
            "Epoch 8/10\n",
            "  Train Loss: 2.4027, Train Acc: 0.4248\n",
            "  Val   Loss: 5.5084, Val   Acc: 0.3495\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.4839, Train Acc: 0.2647\n",
            "  Val   Loss: 4.6728, Val   Acc: 0.0971\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.0572, Train Acc: 0.4575\n",
            "  Val   Loss: 4.6598, Val   Acc: 0.4078\n",
            "Best val macro-F1=0.1452 at epoch 8 | saved: models/best_lr_0.01_inner_512_drop_0.0.pt\n",
            "\n",
            "=== Droprate=0.2 ===\n",
            "  ✅ Saved new best macro-F1=0.0048 to models/best_lr_0.01_inner_512_drop_0.2.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 25.1638, Train Acc: 0.1405\n",
            "  Val   Loss: 23.4391, Val   Acc: 0.0194\n",
            "  ✅ Saved new best macro-F1=0.0140 to models/best_lr_0.01_inner_512_drop_0.2.pt (epoch 2)\n",
            "Epoch 2/10\n",
            "  Train Loss: 20.1663, Train Acc: 0.1569\n",
            "  Val   Loss: 9.7431, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.0285 to models/best_lr_0.01_inner_512_drop_0.2.pt (epoch 3)\n",
            "Epoch 3/10\n",
            "  Train Loss: 7.8429, Train Acc: 0.2059\n",
            "  Val   Loss: 7.7015, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.1371 to models/best_lr_0.01_inner_512_drop_0.2.pt (epoch 4)\n",
            "Epoch 4/10\n",
            "  Train Loss: 4.3311, Train Acc: 0.2255\n",
            "  Val   Loss: 5.9489, Val   Acc: 0.3883\n",
            "Epoch 5/10\n",
            "  Train Loss: 2.9336, Train Acc: 0.3072\n",
            "  Val   Loss: 6.4973, Val   Acc: 0.3592\n",
            "Epoch 6/10\n",
            "  Train Loss: 2.7612, Train Acc: 0.3301\n",
            "  Val   Loss: 7.9288, Val   Acc: 0.4078\n",
            "  ✅ Saved new best macro-F1=0.1613 to models/best_lr_0.01_inner_512_drop_0.2.pt (epoch 7)\n",
            "Epoch 7/10\n",
            "  Train Loss: 2.2817, Train Acc: 0.3039\n",
            "  Val   Loss: 7.3631, Val   Acc: 0.5631\n",
            "Epoch 8/10\n",
            "  Train Loss: 2.0122, Train Acc: 0.3791\n",
            "  Val   Loss: 5.3906, Val   Acc: 0.0874\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.2744, Train Acc: 0.3562\n",
            "  Val   Loss: 5.1155, Val   Acc: 0.3786\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.5118, Train Acc: 0.4150\n",
            "  Val   Loss: 4.4208, Val   Acc: 0.2427\n",
            "Best val macro-F1=0.1613 at epoch 7 | saved: models/best_lr_0.01_inner_512_drop_0.2.pt\n",
            "\n",
            "=== Droprate=0.5 ===\n",
            "  ✅ Saved new best macro-F1=0.0024 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 25.4131, Train Acc: 0.1209\n",
            "  Val   Loss: 15.6552, Val   Acc: 0.0097\n",
            "  ✅ Saved new best macro-F1=0.0049 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 2)\n",
            "Epoch 2/10\n",
            "  Train Loss: 14.2213, Train Acc: 0.2092\n",
            "  Val   Loss: 14.2107, Val   Acc: 0.0194\n",
            "  ✅ Saved new best macro-F1=0.0125 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 3)\n",
            "Epoch 3/10\n",
            "  Train Loss: 8.2887, Train Acc: 0.1928\n",
            "  Val   Loss: 6.2955, Val   Acc: 0.0485\n",
            "  ✅ Saved new best macro-F1=0.1035 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 4)\n",
            "Epoch 4/10\n",
            "  Train Loss: 4.5151, Train Acc: 0.1993\n",
            "  Val   Loss: 4.4371, Val   Acc: 0.2913\n",
            "Epoch 5/10\n",
            "  Train Loss: 3.8538, Train Acc: 0.1667\n",
            "  Val   Loss: 7.2685, Val   Acc: 0.1650\n",
            "  ✅ Saved new best macro-F1=0.1209 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 6)\n",
            "Epoch 6/10\n",
            "  Train Loss: 3.4839, Train Acc: 0.2190\n",
            "  Val   Loss: 4.1902, Val   Acc: 0.2913\n",
            "  ✅ Saved new best macro-F1=0.1609 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 7)\n",
            "Epoch 7/10\n",
            "  Train Loss: 4.1246, Train Acc: 0.2484\n",
            "  Val   Loss: 4.3867, Val   Acc: 0.5049\n",
            "  ✅ Saved new best macro-F1=0.1679 to models/best_lr_0.01_inner_512_drop_0.5.pt (epoch 8)\n",
            "Epoch 8/10\n",
            "  Train Loss: 3.0425, Train Acc: 0.1667\n",
            "  Val   Loss: 3.7572, Val   Acc: 0.4854\n",
            "Epoch 9/10\n",
            "  Train Loss: 3.5655, Train Acc: 0.1928\n",
            "  Val   Loss: 4.0168, Val   Acc: 0.4854\n",
            "Epoch 10/10\n",
            "  Train Loss: 3.2801, Train Acc: 0.2941\n",
            "  Val   Loss: 3.3472, Val   Acc: 0.0874\n",
            "Best val macro-F1=0.1679 at epoch 8 | saved: models/best_lr_0.01_inner_512_drop_0.5.pt\n",
            "\n",
            "=== Droprate=0.8 ===\n",
            "  ✅ Saved new best macro-F1=0.0329 to models/best_lr_0.01_inner_512_drop_0.8.pt (epoch 1)\n",
            "Epoch 1/10\n",
            "  Train Loss: 13.1846, Train Acc: 0.2320\n",
            "  Val   Loss: 7.5208, Val   Acc: 0.0583\n",
            "  ✅ Saved new best macro-F1=0.1069 to models/best_lr_0.01_inner_512_drop_0.8.pt (epoch 2)\n",
            "Epoch 2/10\n",
            "  Train Loss: 13.1324, Train Acc: 0.1307\n",
            "  Val   Loss: 13.9297, Val   Acc: 0.7476\n",
            "Epoch 3/10\n",
            "  Train Loss: 8.2711, Train Acc: 0.1863\n",
            "  Val   Loss: 5.7019, Val   Acc: 0.0291\n",
            "Epoch 4/10\n",
            "  Train Loss: 11.1264, Train Acc: 0.1634\n",
            "  Val   Loss: 5.5336, Val   Acc: 0.1748\n",
            "Epoch 5/10\n",
            "  Train Loss: 9.1087, Train Acc: 0.1928\n",
            "  Val   Loss: 6.5802, Val   Acc: 0.0388\n",
            "Epoch 6/10\n",
            "  Train Loss: 11.6181, Train Acc: 0.1144\n",
            "  Val   Loss: 9.6163, Val   Acc: 0.6796\n",
            "Epoch 7/10\n",
            "  Train Loss: 10.1140, Train Acc: 0.1536\n",
            "  Val   Loss: 6.4795, Val   Acc: 0.1068\n",
            "Epoch 8/10\n",
            "  Train Loss: 9.2842, Train Acc: 0.1536\n",
            "  Val   Loss: 5.5257, Val   Acc: 0.1165\n",
            "  ✅ Saved new best macro-F1=0.1201 to models/best_lr_0.01_inner_512_drop_0.8.pt (epoch 9)\n",
            "Epoch 9/10\n",
            "  Train Loss: 8.4002, Train Acc: 0.2582\n",
            "  Val   Loss: 6.2154, Val   Acc: 0.4175\n",
            "Epoch 10/10\n",
            "  Train Loss: 8.7219, Train Acc: 0.1667\n",
            "  Val   Loss: 4.0730, Val   Acc: 0.0680\n",
            "Best val macro-F1=0.1201 at epoch 9 | saved: models/best_lr_0.01_inner_512_drop_0.8.pt\n"
          ]
        }
      ],
      "source": [
        "inner_size = 512\n",
        "learning_rate = 0.01\n",
        "droprates = [0.0, 0.2, 0.5, 0.8]\n",
        "\n",
        "for droprate in droprates:\n",
        "    print(f\"\\n=== Droprate={droprate} ===\")\n",
        "    model, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner, droprate=droprate)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "\n",
        "    ckpt_name = f\"best_lr_{learning_rate}_inner_{size_inner}_drop_{droprate}.pt\"\n",
        "\n",
        "    best_val_macro_f1, best_epoch, best_path = train_and_evaluate(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        num_epochs=10,\n",
        "        device=device,\n",
        "        ckpt_dir=\"models\",\n",
        "        ckpt_name=ckpt_name\n",
        "    )\n",
        "\n",
        "    print(f\"Best val macro-F1={best_val_macro_f1:.4f} at epoch {best_epoch} | saved: {best_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jr1UKE83YDt"
      },
      "source": [
        "The best droprate is 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwVbEihraFb"
      },
      "source": [
        "### Using the model (single prediction)\n",
        "\n",
        "This predicts from a spectrogram already in your .npy format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrBXbpkprWPq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# If you saved label_map earlier (recommended):\n",
        "# with open(\"data/splits/label_map.json\") as f:\n",
        "#     label_info = json.load(f)\n",
        "# id2label = {int(k): v for k, v in label_info[\"id2label\"].items()}\n",
        "\n",
        "# Quick fallback:\n",
        "id2label = {i: c for i, c in enumerate(sorted(os.listdir(\"data/raw\")))}\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one(spec_np):\n",
        "    \"\"\"\n",
        "    spec_np: numpy array with shape (1, n_mels, time) OR (n_mels, time)\n",
        "    returns: (label, confidence, probs)\n",
        "    \"\"\"\n",
        "    if spec_np.ndim == 2:\n",
        "        spec_np = spec_np[np.newaxis, :, :]\n",
        "    x = torch.tensor(spec_np, dtype=torch.float32).unsqueeze(0).to(device)  # (1,1,mels,time)\n",
        "    logits = model(x)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    return id2label[pred_id], float(probs[pred_id]), probs\n",
        "\n",
        "label, conf, probs = predict_one(X_test[0][0])\n",
        "label, conf\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
