{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKC+lUz/pWGkMZoA007inW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blessingoraz/baby-cry-classifier/blob/main/01_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN for audio files\n",
        "- Do some processing\n",
        "- Check audio lengths/duration and sample rate\n",
        "- How do you detect noise?\n",
        "- Convert audio to images(checkout mel)\n",
        "- Split dataset to training and test\n",
        "\n",
        "Training\n",
        "- Transfer learning\n",
        "- Adjusting learning rate\n",
        "- check-pointing\n",
        "- Regularization and Dropout\n",
        "- Data Augmentation\n",
        "- Training a larger model\n",
        "Using the model"
      ],
      "metadata": {
        "id": "qoHWfbHAxpY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCsK6GRD_eoi",
        "outputId": "13911619-65e0-4f62-905f-302934676bca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "X_train = np.load(\"processed/X_train.npy\")  # (N, 1, n_mels, time)\n",
        "y_train = np.load(\"processed/y_train.npy\")  # (N,)\n",
        "X_val   = np.load(\"processed/X_val.npy\")\n",
        "y_val   = np.load(\"processed/y_val.npy\")\n",
        "X_test  = np.load(\"processed/X_test.npy\")\n",
        "y_test  = np.load(\"processed/y_test.npy\")\n",
        "\n",
        "X_train.shape, y_train.shape\n"
      ],
      "metadata": {
        "id": "4Mb5cJpHA1_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a265a5-88c2-4021-95fe-b3506589af05"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((306, 1, 128, 219), (306,))"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset + DataLoader (with SpecAugment)\n",
        "\n",
        "Weâ€™ll do simple augmentation only in training:\n",
        "\n",
        "Random time masking\n",
        "\n",
        "Random freq masking\n",
        "\n",
        "These work great for spectrograms and are easy."
      ],
      "metadata": {
        "id": "WicY31qFP3ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BabyCryNpyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    - Instead of loading PIL images from disk, we load spectrogram tensors from numpy arrays.\n",
        "    - 'transform' works like torchvision transforms: it modifies the spectrogram before returning it.\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, transform=None):\n",
        "        self.X = X  # numpy array: (N, 1, n_mels, time)\n",
        "        self.y = y  # numpy array: (N,)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spec = torch.tensor(self.X[idx], dtype=torch.float32)  # (1, n_mels, time)\n",
        "        label = torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "        if self.transform:\n",
        "            spec = self.transform(spec)\n",
        "\n",
        "        return spec, label\n"
      ],
      "metadata": {
        "id": "4vsmOAb1oW3q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compose (like torchvision.transforms.Compose)\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for t in self.transforms:\n",
        "            x = t(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Zlfre_31FTpG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple SpecAugment transforms\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "class SpecAugment:\n",
        "    def __init__(self, freq_mask=12, time_mask=24, p=0.7):\n",
        "        self.freq = T.FrequencyMasking(freq_mask_param=freq_mask)\n",
        "        self.time = T.TimeMasking(time_mask_param=time_mask)\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: (1, n_mels, time)\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            x = self.freq(x)\n",
        "            x = self.time(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "1-VpnEQ3FiNu"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize to 224x224\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResizeSpec:\n",
        "    def __init__(self, height=224, width=224):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x: (1, H, W)\n",
        "        x = x.unsqueeze(0)  # (1,1,H,W)\n",
        "        x = F.interpolate(x, size=(self.height, self.width), mode=\"bilinear\", align_corners=False)\n",
        "        return x.squeeze(0)  # (1, height, width)\n"
      ],
      "metadata": {
        "id": "CwKS6tvaFseO"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train/val transforms"
      ],
      "metadata": {
        "id": "2hn-paxKGBY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Train: resize + augmentation\n",
        "\n",
        "Validation: resize only\n",
        "\n",
        "Test: resize only\n",
        "'''\n",
        "train_transforms = Compose([\n",
        "    ResizeSpec(224, 224),     # like transforms.Resize\n",
        "    SpecAugment(freq_mask=12, time_mask=24, p=0.7)  # augmentation\n",
        "])\n",
        "\n",
        "val_transforms = Compose([\n",
        "    ResizeSpec(224, 224)\n",
        "])\n",
        "\n",
        "test_transforms = Compose([\n",
        "    ResizeSpec(224, 224)\n",
        "])\n"
      ],
      "metadata": {
        "id": "Um6EuyNyF8aN"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataLoaders"
      ],
      "metadata": {
        "id": "FnI3CKH2GkNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = BabyCryNpyDataset(X_train, y_train, transform=train_transforms)\n",
        "val_dataset   = BabyCryNpyDataset(X_val, y_val, transform=val_transforms)\n",
        "test_dataset  = BabyCryNpyDataset(X_test, y_test, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# sanity check\n",
        "xb, yb = next(iter(train_loader))\n",
        "xb.shape, yb.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1xhV6MBGjQ1",
        "outputId": "18031f2d-d4ec-4225-f761-f5a94d0478a5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 1, 224, 224]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handle class imbalance (weighted loss)\n",
        "\n",
        "Because the dataset is highly imbalanced, I used class-weighted cross-entropy to penalize misclassification of minority classes more heavily. This encourages the model to learn discriminative features for all cry types rather than overfitting to the majority class."
      ],
      "metadata": {
        "id": "Jhal0Rh_oixX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "counts = Counter(y_train.tolist())\n",
        "num_classes = len(set(y_train.tolist()))\n",
        "\n",
        "class_counts = np.array([counts[i] for i in range(num_classes)], dtype=np.float32)\n",
        "class_weights = class_counts.sum() / (num_classes * class_counts)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "class_weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ib4ZGsloqHr",
        "outputId": "ee51fb41-59c9-419e-9cac-6a082ae02201"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.8250, 3.8250, 7.6500, 2.1250, 0.1678, 5.4643, 3.1875, 2.3906])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, w in enumerate(class_weights):\n",
        "    print(f\"Class {i}: weight={w:.2f}, count={counts[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZxlZiLZKX0Z",
        "outputId": "a8d2972c-2289-41de-e14a-b461588d6631"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: weight=3.83, count=10\n",
            "Class 1: weight=3.83, count=10\n",
            "Class 2: weight=7.65, count=5\n",
            "Class 3: weight=2.12, count=18\n",
            "Class 4: weight=0.17, count=228\n",
            "Class 5: weight=5.46, count=7\n",
            "Class 6: weight=3.19, count=12\n",
            "Class 7: weight=2.39, count=16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning Model (ResNet)\n",
        "\n",
        "I chose ResNet18 as my baseline because it provides a strong balance between model capacity and stability, which is especially important for small, imbalanced datasets. Its residual connections help prevent overfitting and make transfer learning more effective on spectrogram-based audio data.\n",
        "\n",
        "We also add Dropout before the classifier head."
      ],
      "metadata": {
        "id": "_Vv4K27MQXDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CryResNet(nn.Module):\n",
        "    def __init__(self, num_classes, backbone=\"resnet18\", pretrained=True, dropout=0.3, freeze_backbone=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained Resnet18\n",
        "        if backbone == \"resnet18\":\n",
        "            self.base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "        elif backbone == \"resnet34\":\n",
        "            self.base_model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "        else:\n",
        "            raise ValueError(\"backbone must be resnet18 or resnet34\")\n",
        "\n",
        "        # Freeze base model parameters\n",
        "        if freeze_backbone:\n",
        "            for param in self.base_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Replace classifier head\n",
        "        in_features = self.base_model.fc.in_features\n",
        "        self.base_model.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, 1, n_mels, time) -> convert to 3-channel\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        return self.base_model(x)\n"
      ],
      "metadata": {
        "id": "q2-cyf_TBgBS"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Model"
      ],
      "metadata": {
        "id": "WiF1nWwlAzJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = CryResNet(num_classes=num_classes, backbone=\"resnet18\", pretrained=True, dropout=0.3)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.5, patience=2\n",
        ")"
      ],
      "metadata": {
        "id": "lwYeMbSdAynQ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a ReduceLROnPlateau scheduler to automatically lower the learning rate when validation loss stopped improving, allowing for more stable fine-tuning on a small and imbalanced dataset."
      ],
      "metadata": {
        "id": "uG45AyIMLnHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training + validation loops\n",
        "Because the dataset is highly imbalanced, accuracy alone is insufficient. I therefore tracked macro-F1 and per-class recall during validation to ensure that minority cry categories were not ignored.\n",
        "\n",
        "NB: Do this later:\n",
        "- use macro F1\n",
        "- summarize results in a table"
      ],
      "metadata": {
        "id": "hZxVaWlTRTU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def macro_f1_from_logits(logits, labels):\n",
        "    preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
        "    true  = labels.detach().cpu().numpy()\n",
        "    return f1_score(true, preds, average=\"macro\")\n"
      ],
      "metadata": {
        "id": "niQeNCZdzl9g"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def train_and_evaluate(\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    ckpt_dir=\"models\",\n",
        "    ckpt_name=\"best.pt\"):\n",
        "\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "    best_path = os.path.join(ckpt_dir, ckpt_name)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # VALIDATION\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Scheduler watches val_loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # ===== CHECKPOINT =====\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "            torch.save({\n",
        "                \"epoch\": best_epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"best_val_acc\": best_val_acc,\n",
        "            }, best_path)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return best_val_acc, best_epoch, best_path\n"
      ],
      "metadata": {
        "id": "gEI-RoNLBktR"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning the Learning Rate\n",
        "- Try multiple values: [0.0001, 0.001, 0.01, 0.1]\n",
        "- Train for a few epochs each\n",
        "- Compare validation accuracy\n",
        "- Choose the rate with best performance and smallest train/val gap"
      ],
      "metadata": {
        "id": "3pYHtg74Tizk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(learning_rate=1e-3):\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    model = CryResNet(num_classes=num_classes)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=learning_rate,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "\n",
        "    return model, optimizer"
      ],
      "metadata": {
        "id": "bz73t9e3TmWG"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different learning rates:"
      ],
      "metadata": {
        "id": "ApcDjU3CURnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "S1PWHVOptnLw"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f'\\n=== Learning Rate: {lr} ===')\n",
        "    set_seed(42)\n",
        "    model, optimizer = make_model(learning_rate=lr)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "    best_val_acc, best_epoch = train_and_evaluate(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        num_epochs=20,\n",
        "        device=device\n",
        "    )\n",
        "    results.append((lr, best_val_acc, best_epoch))\n",
        "\n",
        "print(\"\\nSummary (best val acc per LR):\")\n",
        "for lr, acc, ep in results:\n",
        "    print(f\"lr={lr:<7} best_val_acc={acc:.4f} at epoch {ep}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLLCyNJJUVJf",
        "outputId": "a941f701-9024-4215-8858-fa916734c7df"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Learning Rate: 0.0001 ===\n",
            "Number of classes: 8\n",
            "Epoch 1/20\n",
            "  Train Loss: 2.2762, Train Acc: 0.0980\n",
            "  Val   Loss: 2.1879, Val   Acc: 0.2621\n",
            "Epoch 2/20\n",
            "  Train Loss: 2.2079, Train Acc: 0.1275\n",
            "  Val   Loss: 2.2277, Val   Acc: 0.4272\n",
            "Epoch 3/20\n",
            "  Train Loss: 2.2493, Train Acc: 0.2353\n",
            "  Val   Loss: 2.2581, Val   Acc: 0.5534\n",
            "Epoch 4/20\n",
            "  Train Loss: 2.2623, Train Acc: 0.1928\n",
            "  Val   Loss: 2.1853, Val   Acc: 0.6505\n",
            "Epoch 5/20\n",
            "  Train Loss: 2.1338, Train Acc: 0.2190\n",
            "  Val   Loss: 2.1666, Val   Acc: 0.5922\n",
            "Epoch 6/20\n",
            "  Train Loss: 2.1276, Train Acc: 0.2288\n",
            "  Val   Loss: 2.1373, Val   Acc: 0.5340\n",
            "Epoch 7/20\n",
            "  Train Loss: 2.1820, Train Acc: 0.1732\n",
            "  Val   Loss: 2.0818, Val   Acc: 0.4563\n",
            "Epoch 8/20\n",
            "  Train Loss: 2.2176, Train Acc: 0.2026\n",
            "  Val   Loss: 2.0263, Val   Acc: 0.3786\n",
            "Epoch 9/20\n",
            "  Train Loss: 2.0726, Train Acc: 0.2157\n",
            "  Val   Loss: 2.0202, Val   Acc: 0.2136\n",
            "Epoch 10/20\n",
            "  Train Loss: 2.1080, Train Acc: 0.2353\n",
            "  Val   Loss: 2.0565, Val   Acc: 0.1553\n",
            "Epoch 11/20\n",
            "  Train Loss: 2.0968, Train Acc: 0.2124\n",
            "  Val   Loss: 2.0793, Val   Acc: 0.1845\n",
            "Epoch 12/20\n",
            "  Train Loss: 2.0709, Train Acc: 0.2092\n",
            "  Val   Loss: 2.0792, Val   Acc: 0.1456\n",
            "Epoch 13/20\n",
            "  Train Loss: 2.1223, Train Acc: 0.1634\n",
            "  Val   Loss: 2.0786, Val   Acc: 0.1359\n",
            "Epoch 14/20\n",
            "  Train Loss: 2.1324, Train Acc: 0.1895\n",
            "  Val   Loss: 2.0783, Val   Acc: 0.1650\n",
            "Epoch 15/20\n",
            "  Train Loss: 2.0436, Train Acc: 0.2059\n",
            "  Val   Loss: 2.0789, Val   Acc: 0.1845\n",
            "Epoch 16/20\n",
            "  Train Loss: 2.0500, Train Acc: 0.2353\n",
            "  Val   Loss: 2.0812, Val   Acc: 0.1650\n",
            "Epoch 17/20\n",
            "  Train Loss: 1.9634, Train Acc: 0.2026\n",
            "  Val   Loss: 2.0777, Val   Acc: 0.1650\n",
            "Epoch 18/20\n",
            "  Train Loss: 2.0186, Train Acc: 0.2059\n",
            "  Val   Loss: 2.0784, Val   Acc: 0.1553\n",
            "Epoch 19/20\n",
            "  Train Loss: 2.0424, Train Acc: 0.2026\n",
            "  Val   Loss: 2.0730, Val   Acc: 0.1456\n",
            "Epoch 20/20\n",
            "  Train Loss: 2.0847, Train Acc: 0.2222\n",
            "  Val   Loss: 2.0785, Val   Acc: 0.1553\n",
            "\n",
            "=== Learning Rate: 0.001 ===\n",
            "Number of classes: 8\n",
            "Epoch 1/20\n",
            "  Train Loss: 2.2411, Train Acc: 0.2026\n",
            "  Val   Loss: 2.1833, Val   Acc: 0.4563\n",
            "Epoch 2/20\n",
            "  Train Loss: 2.0757, Train Acc: 0.1699\n",
            "  Val   Loss: 2.3322, Val   Acc: 0.1748\n",
            "Epoch 3/20\n",
            "  Train Loss: 2.0034, Train Acc: 0.2288\n",
            "  Val   Loss: 2.3390, Val   Acc: 0.3883\n",
            "Epoch 4/20\n",
            "  Train Loss: 2.0114, Train Acc: 0.2157\n",
            "  Val   Loss: 2.2295, Val   Acc: 0.7087\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.7453, Train Acc: 0.3235\n",
            "  Val   Loss: 2.1916, Val   Acc: 0.4466\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.7449, Train Acc: 0.2941\n",
            "  Val   Loss: 2.2553, Val   Acc: 0.3398\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.7974, Train Acc: 0.2222\n",
            "  Val   Loss: 2.1516, Val   Acc: 0.3398\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.7784, Train Acc: 0.3268\n",
            "  Val   Loss: 2.0857, Val   Acc: 0.4951\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.6541, Train Acc: 0.3203\n",
            "  Val   Loss: 2.0614, Val   Acc: 0.2816\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.6248, Train Acc: 0.3235\n",
            "  Val   Loss: 2.1056, Val   Acc: 0.2330\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.5653, Train Acc: 0.3497\n",
            "  Val   Loss: 2.1367, Val   Acc: 0.2136\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.5642, Train Acc: 0.2941\n",
            "  Val   Loss: 2.1142, Val   Acc: 0.2233\n",
            "Epoch 13/20\n",
            "  Train Loss: 1.5998, Train Acc: 0.3170\n",
            "  Val   Loss: 2.1217, Val   Acc: 0.2330\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.5686, Train Acc: 0.3562\n",
            "  Val   Loss: 2.1366, Val   Acc: 0.3301\n",
            "Epoch 15/20\n",
            "  Train Loss: 1.4734, Train Acc: 0.3464\n",
            "  Val   Loss: 2.1373, Val   Acc: 0.2718\n",
            "Epoch 16/20\n",
            "  Train Loss: 1.4693, Train Acc: 0.3824\n",
            "  Val   Loss: 2.1380, Val   Acc: 0.2621\n",
            "Epoch 17/20\n",
            "  Train Loss: 1.4655, Train Acc: 0.3399\n",
            "  Val   Loss: 2.1248, Val   Acc: 0.2427\n",
            "Epoch 18/20\n",
            "  Train Loss: 1.4762, Train Acc: 0.3072\n",
            "  Val   Loss: 2.1245, Val   Acc: 0.2621\n",
            "Epoch 19/20\n",
            "  Train Loss: 1.4949, Train Acc: 0.3235\n",
            "  Val   Loss: 2.1224, Val   Acc: 0.2718\n",
            "Epoch 20/20\n",
            "  Train Loss: 1.5169, Train Acc: 0.3725\n",
            "  Val   Loss: 2.1353, Val   Acc: 0.2913\n",
            "\n",
            "=== Learning Rate: 0.01 ===\n",
            "Number of classes: 8\n",
            "Epoch 1/20\n",
            "  Train Loss: 3.1956, Train Acc: 0.1503\n",
            "  Val   Loss: 3.0707, Val   Acc: 0.0194\n",
            "Epoch 2/20\n",
            "  Train Loss: 2.3845, Train Acc: 0.2026\n",
            "  Val   Loss: 3.4303, Val   Acc: 0.2330\n",
            "Epoch 3/20\n",
            "  Train Loss: 2.3877, Train Acc: 0.2876\n",
            "  Val   Loss: 4.9983, Val   Acc: 0.2039\n",
            "Epoch 4/20\n",
            "  Train Loss: 2.1627, Train Acc: 0.3007\n",
            "  Val   Loss: 3.2625, Val   Acc: 0.6893\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.2276, Train Acc: 0.2549\n",
            "  Val   Loss: 3.7749, Val   Acc: 0.0583\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.1828, Train Acc: 0.5425\n",
            "  Val   Loss: 2.9833, Val   Acc: 0.1748\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.1308, Train Acc: 0.2712\n",
            "  Val   Loss: 3.4213, Val   Acc: 0.6893\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.1717, Train Acc: 0.5621\n",
            "  Val   Loss: 2.9377, Val   Acc: 0.1650\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.1432, Train Acc: 0.4052\n",
            "  Val   Loss: 3.4329, Val   Acc: 0.4854\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.1932, Train Acc: 0.4542\n",
            "  Val   Loss: 3.0065, Val   Acc: 0.2233\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.8662, Train Acc: 0.4542\n",
            "  Val   Loss: 3.3070, Val   Acc: 0.3592\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.0013, Train Acc: 0.4477\n",
            "  Val   Loss: 3.2020, Val   Acc: 0.3301\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.7408, Train Acc: 0.5882\n",
            "  Val   Loss: 3.1013, Val   Acc: 0.3883\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.8315, Train Acc: 0.5588\n",
            "  Val   Loss: 3.1357, Val   Acc: 0.3204\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.9190, Train Acc: 0.4510\n",
            "  Val   Loss: 3.1899, Val   Acc: 0.4369\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.8231, Train Acc: 0.5359\n",
            "  Val   Loss: 3.1834, Val   Acc: 0.3495\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.9595, Train Acc: 0.5000\n",
            "  Val   Loss: 3.0804, Val   Acc: 0.3592\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.8076, Train Acc: 0.4412\n",
            "  Val   Loss: 3.0533, Val   Acc: 0.3107\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.8546, Train Acc: 0.4804\n",
            "  Val   Loss: 3.1328, Val   Acc: 0.3689\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.8937, Train Acc: 0.5163\n",
            "  Val   Loss: 3.1775, Val   Acc: 0.3883\n",
            "\n",
            "=== Learning Rate: 0.1 ===\n",
            "Number of classes: 8\n",
            "Epoch 1/20\n",
            "  Train Loss: 30.4530, Train Acc: 0.1275\n",
            "  Val   Loss: 31.0449, Val   Acc: 0.0194\n",
            "Epoch 2/20\n",
            "  Train Loss: 19.6862, Train Acc: 0.1961\n",
            "  Val   Loss: 28.0590, Val   Acc: 0.0291\n",
            "Epoch 3/20\n",
            "  Train Loss: 14.6260, Train Acc: 0.3235\n",
            "  Val   Loss: 23.1052, Val   Acc: 0.0583\n",
            "Epoch 4/20\n",
            "  Train Loss: 13.7736, Train Acc: 0.4020\n",
            "  Val   Loss: 16.9763, Val   Acc: 0.0388\n",
            "Epoch 5/20\n",
            "  Train Loss: 9.1062, Train Acc: 0.2451\n",
            "  Val   Loss: 33.8595, Val   Acc: 0.2816\n",
            "Epoch 6/20\n",
            "  Train Loss: 10.9037, Train Acc: 0.3660\n",
            "  Val   Loss: 26.4334, Val   Acc: 0.0485\n",
            "Epoch 7/20\n",
            "  Train Loss: 9.1468, Train Acc: 0.4248\n",
            "  Val   Loss: 21.2348, Val   Acc: 0.3107\n",
            "Epoch 8/20\n",
            "  Train Loss: 7.7785, Train Acc: 0.4641\n",
            "  Val   Loss: 18.5771, Val   Acc: 0.2330\n",
            "Epoch 9/20\n",
            "  Train Loss: 7.0460, Train Acc: 0.3824\n",
            "  Val   Loss: 23.6114, Val   Acc: 0.2621\n",
            "Epoch 10/20\n",
            "  Train Loss: 7.8195, Train Acc: 0.5425\n",
            "  Val   Loss: 19.8504, Val   Acc: 0.2330\n",
            "Epoch 11/20\n",
            "  Train Loss: 5.6797, Train Acc: 0.3627\n",
            "  Val   Loss: 23.9406, Val   Acc: 0.5728\n",
            "Epoch 12/20\n",
            "  Train Loss: 6.1077, Train Acc: 0.3660\n",
            "  Val   Loss: 22.0820, Val   Acc: 0.3204\n",
            "Epoch 13/20\n",
            "  Train Loss: 3.3820, Train Acc: 0.5915\n",
            "  Val   Loss: 21.1423, Val   Acc: 0.4078\n",
            "Epoch 14/20\n",
            "  Train Loss: 5.2979, Train Acc: 0.5098\n",
            "  Val   Loss: 20.9926, Val   Acc: 0.3495\n",
            "Epoch 15/20\n",
            "  Train Loss: 4.5164, Train Acc: 0.3562\n",
            "  Val   Loss: 20.7505, Val   Acc: 0.3981\n",
            "Epoch 16/20\n",
            "  Train Loss: 3.8975, Train Acc: 0.5033\n",
            "  Val   Loss: 21.9236, Val   Acc: 0.5340\n",
            "Epoch 17/20\n",
            "  Train Loss: 4.5551, Train Acc: 0.5065\n",
            "  Val   Loss: 20.3889, Val   Acc: 0.4175\n",
            "Epoch 18/20\n",
            "  Train Loss: 3.8173, Train Acc: 0.4542\n",
            "  Val   Loss: 20.1704, Val   Acc: 0.4175\n",
            "Epoch 19/20\n",
            "  Train Loss: 3.4861, Train Acc: 0.5098\n",
            "  Val   Loss: 21.0092, Val   Acc: 0.4272\n",
            "Epoch 20/20\n",
            "  Train Loss: 3.7923, Train Acc: 0.4967\n",
            "  Val   Loss: 21.1392, Val   Acc: 0.4951\n",
            "\n",
            "Summary (best val acc per LR):\n",
            "lr=0.0001  best_val_acc=0.6505 at epoch 4\n",
            "lr=0.001   best_val_acc=0.7087 at epoch 4\n",
            "lr=0.01    best_val_acc=0.6893 at epoch 4\n",
            "lr=0.1     best_val_acc=0.5728 at epoch 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best learning rate is 0.01 (accuracy 0.7087)."
      ],
      "metadata": {
        "id": "1s4jKmhWmw1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Adding Inner Layers"
      ],
      "metadata": {
        "id": "qhsLangtRxOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ResNet, the model already does pooling + flatten internally, so you can just replace the fc head with a small MLP (inner layer + ReLU + dropout + output)."
      ],
      "metadata": {
        "id": "U4kokXCnr75H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CryResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes,\n",
        "        backbone=\"resnet18\",\n",
        "        pretrained=True,\n",
        "        freeze_backbone=True,\n",
        "        size_inner=256,       # <- inner layer size\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load backbone\n",
        "        if backbone == \"resnet18\":\n",
        "            self.base_model = models.resnet18(\n",
        "                weights=models.ResNet18_Weights.DEFAULT if pretrained else None\n",
        "            )\n",
        "        elif backbone == \"resnet34\":\n",
        "            self.base_model = models.resnet34(\n",
        "                weights=models.ResNet34_Weights.DEFAULT if pretrained else None\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"backbone must be resnet18 or resnet34\")\n",
        "\n",
        "        # Freeze backbone\n",
        "        if freeze_backbone:\n",
        "            for p in self.base_model.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # Replace classifier head with: inner -> ReLU -> output\n",
        "        in_features = self.base_model.fc.in_features  # resnet18 => usually 512\n",
        "\n",
        "        self.base_model.fc = nn.Sequential(\n",
        "            nn.Linear(in_features, size_inner),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(size_inner, num_classes)\n",
        "        )\n",
        "\n",
        "        # Ensure head is trainable (safe even if freeze_backbone=True)\n",
        "        for p in self.base_model.fc.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B,1,224,224) -> (B,3,224,224)\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "        return self.base_model(x)\n"
      ],
      "metadata": {
        "id": "KP5htbH7r9Dm"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def make_model(learning_rate=0.01, size_inner=256, backbone=\"resnet18\"):\n",
        "    model = CryResNet(\n",
        "        num_classes=num_classes,\n",
        "        backbone=backbone,\n",
        "        pretrained=True,\n",
        "        freeze_backbone=True,\n",
        "        size_inner=size_inner,\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=learning_rate,\n",
        "        weight_decay=1e-4\n",
        "    )\n",
        "    return model, optimizer\n"
      ],
      "metadata": {
        "id": "sslYpmTisRzu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inner_sizes = [64, 128, 256, 512]\n",
        "learning_rate = 0.01\n",
        "\n",
        "for size_inner in inner_sizes:\n",
        "    print(f\"\\n=== inner={size_inner} ===\")\n",
        "    model, optimizer = make_model(learning_rate=learning_rate, size_inner=size_inner)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
        "\n",
        "    best_val_acc, best_epoch, best_path = train_and_evaluate(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        num_epochs=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f\"Best val acc={best_val_acc:.4f} at epoch {best_epoch} | saved: {best_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxyyASXXs8WJ",
        "outputId": "8774dabc-7cfa-414a-d928-c3a7ea5ef4eb"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== inner=64 ===\n",
            "Epoch 1/10\n",
            "  Train Loss: 3.0428, Train Acc: 0.1176\n",
            "  Val   Loss: 2.0387, Val   Acc: 0.0291\n",
            "Epoch 2/10\n",
            "  Train Loss: 2.0787, Train Acc: 0.0556\n",
            "  Val   Loss: 2.0371, Val   Acc: 0.0583\n",
            "Epoch 3/10\n",
            "  Train Loss: 2.0816, Train Acc: 0.0523\n",
            "  Val   Loss: 2.0643, Val   Acc: 0.0583\n",
            "Epoch 4/10\n",
            "  Train Loss: 2.0611, Train Acc: 0.0458\n",
            "  Val   Loss: 2.0735, Val   Acc: 0.0388\n",
            "Epoch 5/10\n",
            "  Train Loss: 2.0871, Train Acc: 0.0425\n",
            "  Val   Loss: 2.0744, Val   Acc: 0.0388\n",
            "Epoch 6/10\n",
            "  Train Loss: 2.0783, Train Acc: 0.0359\n",
            "  Val   Loss: 2.0750, Val   Acc: 0.0388\n",
            "Epoch 7/10\n",
            "  Train Loss: 2.0897, Train Acc: 0.0392\n",
            "  Val   Loss: 2.0499, Val   Acc: 0.0485\n",
            "Epoch 8/10\n",
            "  Train Loss: 2.0614, Train Acc: 0.0359\n",
            "  Val   Loss: 1.9832, Val   Acc: 0.0388\n",
            "Epoch 9/10\n",
            "  Train Loss: 2.0561, Train Acc: 0.0359\n",
            "  Val   Loss: 1.9700, Val   Acc: 0.0485\n",
            "Epoch 10/10\n",
            "  Train Loss: 2.0533, Train Acc: 0.0392\n",
            "  Val   Loss: 2.0832, Val   Acc: 0.3301\n",
            "Best val acc=0.3301 at epoch 10 | saved: models/best.pt\n",
            "\n",
            "=== inner=128 ===\n",
            "Epoch 1/10\n",
            "  Train Loss: 3.4509, Train Acc: 0.1601\n",
            "  Val   Loss: 2.6312, Val   Acc: 0.0194\n",
            "Epoch 2/10\n",
            "  Train Loss: 2.1236, Train Acc: 0.1863\n",
            "  Val   Loss: 2.0962, Val   Acc: 0.0485\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.8881, Train Acc: 0.2974\n",
            "  Val   Loss: 2.1809, Val   Acc: 0.0583\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.6892, Train Acc: 0.1732\n",
            "  Val   Loss: 2.2122, Val   Acc: 0.0777\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.5305, Train Acc: 0.2255\n",
            "  Val   Loss: 2.3352, Val   Acc: 0.3981\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.2959, Train Acc: 0.5948\n",
            "  Val   Loss: 2.2737, Val   Acc: 0.0291\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.0996, Train Acc: 0.1863\n",
            "  Val   Loss: 2.3545, Val   Acc: 0.0777\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.1672, Train Acc: 0.2810\n",
            "  Val   Loss: 2.6913, Val   Acc: 0.3689\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.1038, Train Acc: 0.5948\n",
            "  Val   Loss: 2.3919, Val   Acc: 0.2330\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.1055, Train Acc: 0.3137\n",
            "  Val   Loss: 2.4204, Val   Acc: 0.0680\n",
            "Best val acc=0.3981 at epoch 5 | saved: models/best.pt\n",
            "\n",
            "=== inner=256 ===\n",
            "Epoch 1/10\n",
            "  Train Loss: 5.1271, Train Acc: 0.1830\n",
            "  Val   Loss: 2.2974, Val   Acc: 0.0388\n",
            "Epoch 2/10\n",
            "  Train Loss: 2.1805, Train Acc: 0.0294\n",
            "  Val   Loss: 2.0881, Val   Acc: 0.0194\n",
            "Epoch 3/10\n",
            "  Train Loss: 2.0335, Train Acc: 0.0359\n",
            "  Val   Loss: 2.1839, Val   Acc: 0.0194\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.9826, Train Acc: 0.0621\n",
            "  Val   Loss: 2.1325, Val   Acc: 0.0097\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.9215, Train Acc: 0.0654\n",
            "  Val   Loss: 2.1042, Val   Acc: 0.0291\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.8931, Train Acc: 0.0556\n",
            "  Val   Loss: 2.1662, Val   Acc: 0.0194\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.7965, Train Acc: 0.0621\n",
            "  Val   Loss: 2.1363, Val   Acc: 0.0194\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.7144, Train Acc: 0.0784\n",
            "  Val   Loss: 2.3434, Val   Acc: 0.0194\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.6343, Train Acc: 0.1078\n",
            "  Val   Loss: 2.2997, Val   Acc: 0.0485\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.5760, Train Acc: 0.1373\n",
            "  Val   Loss: 2.2618, Val   Acc: 0.1262\n",
            "Best val acc=0.1262 at epoch 10 | saved: models/best.pt\n",
            "\n",
            "=== inner=512 ===\n",
            "Epoch 1/10\n",
            "  Train Loss: 8.4991, Train Acc: 0.1634\n",
            "  Val   Loss: 2.5901, Val   Acc: 0.7184\n",
            "Epoch 2/10\n",
            "  Train Loss: 2.2390, Train Acc: 0.1503\n",
            "  Val   Loss: 2.1343, Val   Acc: 0.2816\n",
            "Epoch 3/10\n",
            "  Train Loss: 2.1279, Train Acc: 0.0882\n",
            "  Val   Loss: 2.0710, Val   Acc: 0.1748\n",
            "Epoch 4/10\n",
            "  Train Loss: 2.0079, Train Acc: 0.2680\n",
            "  Val   Loss: 2.2256, Val   Acc: 0.0680\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.9577, Train Acc: 0.1242\n",
            "  Val   Loss: 2.3180, Val   Acc: 0.0388\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.8334, Train Acc: 0.1144\n",
            "  Val   Loss: 2.1412, Val   Acc: 0.0291\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.7061, Train Acc: 0.1046\n",
            "  Val   Loss: 2.1763, Val   Acc: 0.0388\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.6581, Train Acc: 0.2647\n",
            "  Val   Loss: 2.2069, Val   Acc: 0.1262\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.4920, Train Acc: 0.2026\n",
            "  Val   Loss: 2.1763, Val   Acc: 0.1942\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.3741, Train Acc: 0.3922\n",
            "  Val   Loss: 2.0384, Val   Acc: 0.3495\n",
            "Best val acc=0.7184 at epoch 1 | saved: models/best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload best model later\n",
        "\n",
        "ckpt = torch.load(\"models/best.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "print(\"Loaded best checkpoint from epoch\", ckpt[\"epoch\"], \"val_acc\", ckpt[\"best_val_acc\"])\n"
      ],
      "metadata": {
        "id": "vU_pHTYwq_tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout Regularization"
      ],
      "metadata": {
        "id": "R3vpXbQTxBfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lx3zNH_NxCxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the model (single prediction)\n",
        "\n",
        "This predicts from a spectrogram already in your .npy format."
      ],
      "metadata": {
        "id": "RNwVbEihraFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# If you saved label_map earlier (recommended):\n",
        "# with open(\"data/splits/label_map.json\") as f:\n",
        "#     label_info = json.load(f)\n",
        "# id2label = {int(k): v for k, v in label_info[\"id2label\"].items()}\n",
        "\n",
        "# Quick fallback:\n",
        "id2label = {i: c for i, c in enumerate(sorted(os.listdir(\"data/raw\")))}\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_one(spec_np):\n",
        "    \"\"\"\n",
        "    spec_np: numpy array with shape (1, n_mels, time) OR (n_mels, time)\n",
        "    returns: (label, confidence, probs)\n",
        "    \"\"\"\n",
        "    if spec_np.ndim == 2:\n",
        "        spec_np = spec_np[np.newaxis, :, :]\n",
        "    x = torch.tensor(spec_np, dtype=torch.float32).unsqueeze(0).to(device)  # (1,1,mels,time)\n",
        "    logits = model(x)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    return id2label[pred_id], float(probs[pred_id]), probs\n",
        "\n",
        "label, conf, probs = predict_one(X_test[0][0])\n",
        "label, conf\n"
      ],
      "metadata": {
        "id": "qrBXbpkprWPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}